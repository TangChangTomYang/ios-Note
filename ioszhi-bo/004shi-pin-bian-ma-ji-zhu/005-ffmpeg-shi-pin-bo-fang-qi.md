###iOSFFmpeg 的使用
参考文章： https://www.cnblogs.com/XYQ-208910/p/5651166.html
**现状：**
现在视频直播非常的火，所以在视频直播的开发中，使用的对视频进行编码的框架显得尤为的重要，其实这种框架有很多，现在主要介绍一下FFMpeg 视频播放器的集成和使用，FFMpeg 是视频编解码的利器。

####一、介绍视频播放的过程


**首先介绍一下视频文件的相关知识。**
我们平时看到的视频文件有许多的格式，比如：**avi、mkv、rmvb、mov、MP4等等**，这些都被称为**容器container**，**不同的容器格式规定了其中音视频数据的组织方式（也包括其它数据，比如字幕等等）**。容器中一般会封装有**视频和音频轨道，也称为视频流和音频流（stream）**,播放视频文件的第一步就是根据视频文件的格式，解析（demux）出其中封装的视频流、音频流以及字幕（如果有的话），解析的数据包（packet）中，每个包里保存的是视频帧（frame）或音频镇，然后分别对视频帧和音频帧调用相应的解码器(decoder)进行解码，解码之后得到的就是原始的图像（YUV or RGB）和声音（PCM）数据，然后根据同步好的时间将图像显示到屏幕上，将声音输出到声卡上，最终就是我们看到的视频。

FFMpeg的API就是根据这个过程设计的，因此使用FFmpeg 来处理视频文件的方法非常的直观简单、
**下面就一步一步介绍从视频文件中解码出图谱按的过程：**

属性声明：
- **AVFormatContext**: 保存需要读入的文件的格式信息，比如：流的个数以及流数据等。
- **AVCoderContext**: 保存了相应流的详细编码信息，比如： 视频的宽、高，编码类型等。
- **pCodec**: 真正的编解码器，其中有编解码需要调用的函数。
- **AVFrame**:用于保存数据帧的数据结构，这里的两个帧分别是保存颜色转换前后的两帧图像。
- **AVpacket**：解析文件时会将音/视频帧读入到packet中

####二、播放器原理
- 通过FFmpeg对视频进行解码，解码出每一帧的图片，然后根据一定的播放时间播放每一帧图。

####三、如何集成FFmpeg
- [下载FFmpeg脚本](https://github.com/kewlbear/FFmpeg-iOS-build-script)
- 根据上面链接的 README 进行编译

**大致步骤：**
    - 1.下载脚本https://github.com/kewlbear/FFmpeg-iOS-build-script
    - 解压，找到文件 build-ffmpeg.sh
    - 进入终端，执行脚本文件 ./build-ffmpeg
    这是编译后的静态库，截图如下：
    ![](/assets/791499-20160711162310998-941764484.png)
    - 集成到项目,新建工程,将编译好的静态库以及头文件导入工程(demo)
    ![](/assets/791499-20160707182426905-1434712610.png)
    - 导入依赖库
    ![](/assets/791499-20160707182026108-1545606509.png)
    - 设置头文件路径,路径一定要对,不然胡找不到头文件
    ![](/assets/791499-20160707182108046-1645192806.png)
    我设置路径如下图：

![](/assets/791499-20160711170150623-67882242-2.png)
- 先 command + B 编译一下,确保能编译成功































